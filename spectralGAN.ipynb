{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tfplot\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras as k\n",
    "from tensorflow import layers as ly\n",
    "\n",
    "from tqdm import tqdm_notebook as pbar\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum(spec):\n",
    "    fig, ax = tfplot.subplots(figsize=(4, 3))\n",
    "    im = ax.plot(np.arange(1180, 1280, 0.25), spec)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_parameters():\n",
    "    total_parameters = 0\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dim in shape:\n",
    "            variable_parameters *= dim.value\n",
    "        total_parameters += variable_parameters\n",
    "    return total_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x):\n",
    "    \n",
    "    y = ly.conv1d(\n",
    "        inputs=x,\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        activation=tf.identity\n",
    "        )\n",
    "    y = k.layers.PReLU()(y)\n",
    "    y = ly.conv1d(\n",
    "        inputs=y,\n",
    "        filters=64,\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        activation=tf.identity\n",
    "        )\n",
    "    \n",
    "    return tf.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subpixel_convolution(x, block_size):\n",
    "    \n",
    "    y = ly.conv1d(\n",
    "    inputs=x,\n",
    "    filters=256,\n",
    "    kernel_size=3,\n",
    "    strides=1,\n",
    "    padding='same',\n",
    "    kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "    activation=tf.identity\n",
    "    )\n",
    "    y = tf.depth_to_space(y, block_size=block_size)\n",
    "    \n",
    "    return k.layers.PReLU()(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(x, filters, stride):\n",
    "    \n",
    "    y = ly.conv1d(\n",
    "    inputs=x,\n",
    "    filters=filters,\n",
    "    kernel_size=3,\n",
    "    strides=stride,\n",
    "    padding='same',\n",
    "    kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "    activation=tf.identity\n",
    "    )\n",
    "    y = ly.batch_normalization(y)\n",
    "    \n",
    "    return k.layers.LeakyReLU(alpha=0.2)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReInitDataSampler:\n",
    "    def __init__(self, train_filepath, batch_size, valid_filepath=None, test_filepath=None, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        train_files = [os.path.join(train_filepath, file) for file in os.listdir(train_filepath) if file.endswith('.tfrecords')]\n",
    "        train_dataset = self.make_dataset(train_files)\n",
    "        self.iter = tf.data.Iterator.from_structure(train_dataset.output_types, train_dataset.output_shapes)\n",
    "        self.train_init_op = self.iter.make_initializer(train_dataset)\n",
    "\n",
    "        if valid_filepath is not None:\n",
    "            valid_files = [os.path.join(valid_filepath, file) for file in os.listdir(valid_filepath) if file.endswith('.tfrecords')]\n",
    "            valid_dataset = self.make_dataset(valid_files)\n",
    "            self.valid_init_op = self.iter.make_initializer(valid_dataset)\n",
    "\n",
    "        if test_filepath is not None:\n",
    "            test_files = [os.path.join(test_filepath, file) for file in os.listdir(test_filepath) if file.endswith('.tfrecords')]\n",
    "            test_dataset = self.make_test_dataset(test_files)\n",
    "            self.test_init_op = self.iter.make_initializer(test_dataset)\n",
    "    \n",
    "    def make_dataset(self, files):\n",
    "        dataset = tf.data.TFRecordDataset(files).map(self.decoder)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "        \n",
    "        return dataset.repeat().batch(self.batch_size)\n",
    "    \n",
    "    def make_test_dataset(self, files):\n",
    "        dataset = tf.data.TFRecordDataset(files).map(self.decoder)\n",
    "        return dataset.batch(self.batch_size)\n",
    "    \n",
    "    def initialize(self, dataset='train'):\n",
    "        if dataset == 'train':\n",
    "            return self.train_init_op\n",
    "        elif dataset == 'valid':\n",
    "            return self.valid_init_op\n",
    "        elif dataset == 'test':\n",
    "            return self.test_init_op\n",
    "        else:\n",
    "            raise ValueError('Dataset unknown or unavailable.')\n",
    "\n",
    "    def decoder(self, example_proto):\n",
    "        keys_to_features = {'latent' : tf.FixedLenFeature(4000, tf.float32),\n",
    "                            'target' : tf.FixedLenFeature(400, tf.float32),\n",
    "                            'metadata' : tf.FixedLenFeature(4, tf.float32)}\n",
    "        parsed_features = tf.parse_single_example(example_proto, keys_to_features)\n",
    "        return parsed_features['latent'], parsed_features['target'], parsed_features['metadata']\n",
    "\n",
    "    def get_batch(self):\n",
    "        x, y, z = self.iter.get_next()\n",
    "        x = tf.reshape(x, [-1, 4000, 1])\n",
    "        y = tf.reshape(y, [-1, 400, 1])\n",
    "        z = tf.reshape(z, [-1, 4])\n",
    "        return x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator:\n",
    "    def __init__(self, name='discriminator'):\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, x, reuse=False):\n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "            if reuse:\n",
    "                vs.reuse_variables()\n",
    "            \n",
    "            y = ly.conv1d(\n",
    "                inputs=x,\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=1,\n",
    "                padding='same',\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation=tf.identity\n",
    "                )\n",
    "            y = k.layers.LeakyReLU(alpha=0.2)(y)\n",
    "            \n",
    "            y = convolution_block(y, filters=64, stride=2)\n",
    "            y = convolution_block(y, filters=128, stride=1)\n",
    "            y = convolution_block(y, filters=128, stride=2)\n",
    "            y = convolution_block(y, filters=256, stride=1)\n",
    "            y = convolution_block(y, filters=256, stride=2)\n",
    "            y = convolution_block(y, filters=512, stride=1)\n",
    "            y = convolution_block(y, filters=512, stride=2)\n",
    "            \n",
    "            y = ly.Flatten()(y)\n",
    "    \n",
    "            y = ly.dense(\n",
    "                inputs=y,\n",
    "                units=512,\n",
    "                activation=tf.identity,\n",
    "                )\n",
    "            y = k.layers.LeakyReLU(alpha=0.2)(y)\n",
    "            y = ly.dense(\n",
    "                inputs=y,\n",
    "                units=1,\n",
    "                activation=tf.identity,\n",
    "                )\n",
    "            \n",
    "            return y\n",
    "            \n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, num_blocks=8, name='generator'):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, x):\n",
    "        with tf.variable_scope(self.name) as vs:\n",
    "\n",
    "            y = ly.conv1d(\n",
    "                inputs=x,\n",
    "                filters=64,\n",
    "                kernel_size=9,\n",
    "                strides=1,\n",
    "                padding='same',\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation=tf.identity\n",
    "                )\n",
    "            y = k.layers.PReLU()(y)\n",
    "            y_ = tf.identity(y)\n",
    "\n",
    "            for i in range(self.num_blocks):\n",
    "                y = residual_block(y)\n",
    "            \n",
    "            y = tf.add(y, y_)\n",
    "            \n",
    "            y = ly.conv1d(\n",
    "                inputs=y,\n",
    "                filters=64,\n",
    "                kernel_size=5,\n",
    "                strides=4,\n",
    "                padding='same',\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation=tf.identity\n",
    "                )\n",
    "            \n",
    "            y = ly.conv1d(\n",
    "                inputs=y,\n",
    "                filters=64,\n",
    "                kernel_size=5,\n",
    "                strides=4,\n",
    "                padding='same',\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation=tf.identity\n",
    "                )\n",
    "            \n",
    "            y = ly.conv1d(\n",
    "                inputs=y,\n",
    "                filters=64,\n",
    "                kernel_size=3,\n",
    "                strides=2,\n",
    "                padding='same',\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "                activation=tf.identity\n",
    "                )\n",
    "\n",
    "            y = ly.Flatten()(y)\n",
    "            \n",
    "            y = ly.dense(\n",
    "                inputs=y,\n",
    "                units=512,\n",
    "                activation=tf.identity,\n",
    "                )\n",
    "            y = k.layers.PReLU()(y)\n",
    "            y = ly.dense(\n",
    "                inputs=y,\n",
    "                units=400,\n",
    "                activation=tf.tanh,\n",
    "                )\n",
    "            \n",
    "            y = tf.reshape(y, [-1, 400, 1])\n",
    "\n",
    "            return y\n",
    "\n",
    "    @property\n",
    "    def vars(self):\n",
    "        return [var for var in tf.global_variables() if self.name in var.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, generator, discriminator, data_sampler, logdir, ckptdir):\n",
    "        self.g = generator\n",
    "        self.d = discriminator\n",
    "        self.ds = data_sampler\n",
    "        self.logdir = logdir\n",
    "        self.ckptdir = ckptdir\n",
    "\n",
    "        self.latent, self.target, self.metadata = data_sampler.get_batch()\n",
    "\n",
    "        self.target_ = self.g(self.latent)\n",
    "\n",
    "        logits_real = self.d(self.target)\n",
    "        logits_fake = self.d(self.target_, reuse=True)\n",
    "\n",
    "        d_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(logits_real), logits_real)\n",
    "        d_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(logits_fake), logits_fake)\n",
    "        self.d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        self.adv_loss = 1e-2*tf.losses.sigmoid_cross_entropy(tf.ones_like(logits_fake), logits_fake)\n",
    "        self.mse_loss = tf.losses.mean_squared_error(self.target_, self.target)\n",
    "        self.g_loss = self.mse_loss + self.adv_loss\n",
    "\n",
    "        self.lr = tf.Variable(1e-4, trainable=False)\n",
    "\n",
    "        self.d_adam, self.g_adam = None, None\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_adam = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9).minimize(self.d_loss, var_list=self.d.vars)\n",
    "            self.g_adam = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.9).minimize(self.g_loss, var_list=self.g.vars)\n",
    "\n",
    "        if not os.path.isdir(logdir):\n",
    "            os.makedirs(logdir)\n",
    "\n",
    "        if not os.path.isdir(ckptdir):\n",
    "            os.makedirs(ckptdir)\n",
    "\n",
    "        tfplot.summary.plot(\"Generated_Spectrum\", plot_spectrum, [tf.reshape(self.target_[0], [400])])\n",
    "        d_loss_summary = tf.summary.scalar(\"D_Loss\", self.d_loss)\n",
    "        g_loss_summary = tf.summary.scalar(\"G_Total_Loss\", self.g_loss)\n",
    "        adv_loss_summary = tf.summary.scalar(\"G_Adv_Loss\", self.adv_loss)\n",
    "        mse_loss_summary = tf.summary.scalar(\"G_MSE_Loss\", self.mse_loss)\n",
    "        lr_summary = tf.summary.scalar(\"Learning_Rate\", self.lr)\n",
    "\n",
    "        self.merged_summary = tf.summary.merge_all()\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(logdir)\n",
    "        self.saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "        self.config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "    def train(self, batches=int(1e6), restore=False):\n",
    "        \n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            if restore:\n",
    "                meta_graph = [os.path.join(self.ckptdir, file) for file in \\\n",
    "                              os.listdir(self.ckptdir) if file.endswith('.meta')][0]\n",
    "                restorer = tf.train.import_meta_graph(meta_graph)\n",
    "                restorer.restore(sess, tf.train.latest_checkpoint(self.ckptdir))\n",
    "            \n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                print(\"{:,} global variables initialized.\".format(get_total_parameters()))\n",
    "                \n",
    "            sess.run(self.ds.initialize('train'))\n",
    "            print(\"Data sampler initialized on train dataset.\")\n",
    "            \n",
    "            try:\n",
    "                for batch in pbar(range(batches), unit='batch'):\n",
    "                    sess.run(self.d_adam)\n",
    "                    sess.run(self.g_adam)\n",
    "                    \n",
    "                    if batch % 100 == 0:\n",
    "                        summaries = sess.run(self.merged_summary)\n",
    "                        self.summary_writer.add_summary(summaries, batch)\n",
    "\n",
    "                    if batch % 10000 == 0 or batch + 1 == batches:\n",
    "                        self.saver.save(sess, os.path.join(self.ckptdir, 'ckpt'), global_step=batch+1)\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Saving model before quitting...\")\n",
    "                self.saver.save(sess, os.path.join(self.ckptdir, 'ckpt'), global_step=batch+1)\n",
    "                print(\"Save complete. Training stopped.\")\n",
    "                    \n",
    "    def infer(self, n, savedir='/vol/data/spectralgan/predictions-latent'):\n",
    "\n",
    "        def plot(flux1, flux2, latent, metadata, i):\n",
    "            flux1, flux2 = flux1.reshape([-1, 400]), flux2.reshape([-1, 400])\n",
    "            latent, metadata = latent.reshape([-1, 4000]), metadata.reshape([-1, 4])\n",
    "            wave = np.arange(1180, 1280, 0.25)\n",
    "            wave2 = np.arange(1280, 2280, 0.25)\n",
    "            \n",
    "            for j in range(flux1.shape[0]):\n",
    "                plt.clf()\n",
    "                a, b, c, d = metadata[j]\n",
    "                f1, f2, l = flux1[j], flux2[j], latent[j]\n",
    "                \n",
    "                f1 = (d-c)*(f1 + 1)/2 + c\n",
    "                f2 = (d-c)*(f2 + 1)/2 + c\n",
    "                \n",
    "                l = b*l + a\n",
    "                \n",
    "                plt.plot(wave, f1, label='True Emission')\n",
    "                plt.plot(wave, f2, label='Inferred Emission')\n",
    "                plt.plot(wave2, l, label='Latent Spectrum')\n",
    "                plt.axvline(1215.67, color='r', linestyle='--')\n",
    "                plt.legend()\n",
    "                savepath = os.path.join(savedir, 'pred-batch{}-sample{}.png'.format(i, j))\n",
    "                plt.savefig(savepath, dpi=300, bbox_inches='tight')\n",
    "            \n",
    "        if not os.path.isdir(savedir):\n",
    "            os.makedirs(savedir)\n",
    "            \n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(self.ds.initialize('test'))\n",
    "            print(\"Data sampler initialized on test dataset.\")\n",
    "\n",
    "            meta_graph = [os.path.join(self.ckptdir, file) for file in os.listdir(self.ckptdir) if file.endswith('.meta')][0]\n",
    "            restorer = tf.train.import_meta_graph(meta_graph)\n",
    "            restorer.restore(sess, tf.train.latest_checkpoint(self.ckptdir))\n",
    "            print(\"Restored {:,} global parameters.\".format(get_total_parameters()))\n",
    "            \n",
    "            for i in pbar(range(n)):\n",
    "                latent, target, metadata, target_ = sess.run([self.latent, self.target, self.metadata, self.target_])\n",
    "                plot(target, target_, latent, metadata, i)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ReInitDataSampler(\n",
    "    train_filepath='/vol/data/spectralgan/train',\n",
    "    test_filepath='/vol/data/spectralgan/test',\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = '/vol/projects/spectralgan/logdir'\n",
    "ckptdir = '/vol/projects/spectralgan/ckptdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(generator, discriminator, sampler, logdir, ckptdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /vol/projects/spectralgan/ckptdir/ckpt-40001\n",
      "Data sampler initialized on train dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1158253699bc4b9fa6e52d86cf20a92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gan.train(restore=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sampler initialized on test dataset.\n",
      "INFO:tensorflow:Restoring parameters from /vol/projects/spectralgan/ckptdir/ckpt-30001\n",
      "Restored 44,936,883 global parameters.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a73db4f20c433fa792e3df7fd960e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gan.infer(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
